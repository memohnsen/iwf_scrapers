name: Daily IWF World Records Scrape

on:
  schedule:
    # Run daily at 8:00 AM UTC
    - cron: '0 8 * * *'
  workflow_dispatch:  # Allow manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: ./world-records
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
    
    - name: Run scraper
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
      run: |
        python scraper.py
    
    - name: Upload CSV artifact
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: world-records-csv
        path: world_records_latest.csv
        retention-days: 30

