"""
IWF World Records Scraper
Scrapes world records from IWF website, saves to CSV, upserts to Supabase, and sends Discord notifications

Setup & Usage:
    # Create virtual environment
    python3 -m venv venv
    source venv/bin/activate
    
    # Install dependencies
    pip install -r requirements.txt

    # Run scraper (normal mode - writes to DB)
    python scraper.py
    
    # Run in dry-run mode (compares with DB but doesn't write)
    python scraper.py --dry-run
"""
import requests
from bs4 import BeautifulSoup
import csv
import os
import sys
from datetime import datetime
from typing import List, Dict, Optional
import time
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()


class IWFWorldRecordsScraper:
    """Complete scraper for IWF World Records with Supabase and Discord integration"""
    
    BASE_URL = "https://iwf.sport/results/world-records/"
    
    # Configuration for all age groups and genders to scrape
    CONFIGURATIONS = [
        {"ranking_curprog": "current", "ranking_agegroup": "Senior", "ranking_gender": "m"},
        {"ranking_curprog": "current", "ranking_agegroup": "Senior", "ranking_gender": "w"},
        {"ranking_curprog": "current", "ranking_agegroup": "Junior", "ranking_gender": "w"},
        {"ranking_curprog": "current", "ranking_agegroup": "Junior", "ranking_gender": "m"},
        {"ranking_curprog": "current", "ranking_agegroup": "Youth", "ranking_gender": "m"},
        {"ranking_curprog": "current", "ranking_agegroup": "Youth", "ranking_gender": "w"},
    ]
    
    def __init__(self, dry_run: bool = False):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # Environment variables
        self.supabase_url = os.getenv('SUPABASE_URL')
        self.supabase_key = os.getenv('SUPABASE_KEY')
        self.discord_webhook = os.getenv('DISCORD_WEBHOOK_URL')
        
        # Dry run mode
        self.dry_run = dry_run
        
    def build_url(self, config: Dict[str, str]) -> str:
        """Build URL with query parameters"""
        params = "&".join([f"{k}={v}" for k, v in config.items()])
        return f"{self.BASE_URL}?{params}"
    
    def fetch_page(self, url: str) -> str:
        """Fetch page content"""
        print(f"Fetching: {url}")
        response = self.session.get(url)
        response.raise_for_status()
        return response.text
    
    def parse_record_value(self, text: str) -> Optional[float]:
        """Extract numeric value from record text (e.g., '141 kg' -> 141.0)"""
        if not text or "World Standard" in text:
            return None
        
        try:
            # Remove 'kg' and other text, get just the number
            value = text.strip().replace("kg", "").strip()
            return float(value)
        except (ValueError, AttributeError):
            return None
    
    def parse_weight_class(self, heading_text: str) -> str:
        """Extract weight class from heading (e.g., '60 kg' or '+110 kg')"""
        # Remove space between number and 'kg'
        return heading_text.strip().replace(' kg', 'kg')
    
    def scrape_page(self, config: Dict[str, str]) -> List[Dict]:
        """Scrape a single page and extract records"""
        import re
        
        url = self.build_url(config)
        html_content = self.fetch_page(url)
        soup = BeautifulSoup(html_content, 'html.parser')
        
        records = []
        
        # Determine age category and gender from config
        age_category = config['ranking_agegroup']
        gender = 'Men' if config['ranking_gender'] == 'm' else 'Women'
        
        # Find all weight class sections (div with class 'results__title')
        weight_title_divs = soup.find_all('div', class_='results__title')
        
        for title_div in weight_title_divs:
            # Find the h2 tag with the weight class
            h2 = title_div.find('h2')
            if not h2:
                continue
            
            weight_class_text = h2.get_text(strip=True)
            
            # Skip if not a weight class (should contain 'kg')
            if 'kg' not in weight_class_text.lower():
                continue
            
            weight_class = self.parse_weight_class(weight_class_text)
            
            # Find the next sibling div which contains the records (class='cards')
            cards_div = title_div.find_next_sibling('div', class_='cards')
            
            if not cards_div:
                continue
            
            # Get all text from the cards div
            cards_text = cards_div.get_text()
            
            # Extract all record values using regex
            # Pattern: "Record: XXX kg" where XXX is the number
            record_matches = re.findall(r'Record:\s*(\d+(?:\.\d+)?)\s*kg', cards_text)
            
            # Initialize record values
            snatch_record = None
            cj_record = None
            total_record = None
            
            # The records appear in order: Snatch, C&J, Total
            # Split the text into sections
            snatch_section = cards_text.split('C&J')[0] if 'C&J' in cards_text else ''
            cj_section = cards_text.split('C&J')[1].split('Total')[0] if 'C&J' in cards_text and 'Total' in cards_text else ''
            total_section = cards_text.split('Total')[1] if 'Total' in cards_text else ''
            
            # Extract Snatch record (includes World Standards)
            snatch_match = re.search(r'Record:\s*(\d+(?:\.\d+)?)\s*kg', snatch_section)
            if snatch_match:
                snatch_record = int(float(snatch_match.group(1)))
            
            # Extract C&J record (includes World Standards)
            cj_match = re.search(r'Record:\s*(\d+(?:\.\d+)?)\s*kg', cj_section)
            if cj_match:
                cj_record = int(float(cj_match.group(1)))
            
            # Extract Total record (includes World Standards)
            total_match = re.search(r'Record:\s*(\d+(?:\.\d+)?)\s*kg', total_section)
            if total_match:
                total_record = int(float(total_match.group(1)))
            
            # Add record to list
            record = {
                'age_category': age_category,
                'gender': gender,
                'weight_class': weight_class,
                'snatch_record': snatch_record,
                'cj_record': cj_record,
                'total_record': total_record
            }
            
            records.append(record)
            print(f"  Found: {age_category} {gender} {weight_class} (S:{snatch_record}, C&J:{cj_record}, T:{total_record})")
        
        return records
    
    def scrape_all(self) -> List[Dict]:
        """Scrape all configurations"""
        all_records = []
        
        for config in self.CONFIGURATIONS:
            print(f"\nScraping {config['ranking_agegroup']} {config['ranking_gender'].upper()}...")
            try:
                records = self.scrape_page(config)
                all_records.extend(records)
                time.sleep(1)  # Be nice to the server
            except Exception as e:
                print(f"Error scraping {config}: {e}")
                continue
        
        return all_records
    
    def save_to_csv(self, records: List[Dict], filename: str = 'world_records_latest.csv') -> str:
        """Save records to CSV file"""
        filepath = os.path.join(os.path.dirname(__file__) or '.', filename)
        
        with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['age_category', 'gender', 'weight_class', 
                         'snatch_record', 'cj_record', 'total_record']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            writer.writeheader()
            writer.writerows(records)
        
        print(f"‚úÖ Saved {len(records)} records to {filepath}")
        return filepath
    
    def get_existing_records(self) -> List[Dict]:
        """Fetch existing records from Supabase"""
        if not self.supabase_url or not self.supabase_key:
            return []
        
        try:
            from supabase import create_client
            
            client = create_client(self.supabase_url, self.supabase_key)
            table_name = 'world_records'
            
            result = client.table(table_name).select("*").execute()
            return result.data if hasattr(result, 'data') else []
            
        except Exception as e:
            print(f"Warning: Could not fetch existing records: {e}")
            return []
    
    def compare_records(self, new_records: List[Dict], existing_records: List[Dict]) -> Dict:
        """Compare new records with existing records"""
        # Create lookup key for records
        def make_key(record):
            return (record.get('age_category'), record.get('gender'), record.get('weight_class'))
        
        existing_map = {make_key(r): r for r in existing_records}
        
        changes = {
            'new': [],
            'modified': [],
            'unchanged': []
        }
        
        for new_record in new_records:
            key = make_key(new_record)
            existing = existing_map.get(key)
            
            if not existing:
                changes['new'].append(new_record)
            else:
                # Check if values changed
                if (existing.get('snatch_record') != new_record.get('snatch_record') or
                    existing.get('cj_record') != new_record.get('cj_record') or
                    existing.get('total_record') != new_record.get('total_record')):
                    changes['modified'].append({
                        'record': new_record,
                        'old': existing
                    })
                else:
                    changes['unchanged'].append(new_record)
        
        return changes
    
    def print_dry_run_summary(self, new_records: List[Dict], changes: Dict):
        """Print detailed dry run summary"""
        print("\n" + "=" * 80)
        print("DRY RUN MODE - NO CHANGES WILL BE MADE")
        print("=" * 80)
        
        # Calculate total items that would be upserted (new + modified)
        total_to_upsert = len(changes['new']) + len(changes['modified'])
        
        print(f"\nTotal records scraped: {len(new_records)}")
        print(f"Total records that would be UPSERTED: {total_to_upsert}")
        print(f"  - New records: {len(changes['new'])}")
        print(f"  - Modified records: {len(changes['modified'])}")
        print(f"  - Unchanged records: {len(changes['unchanged'])}")
        
        if changes['new']:
            print("\n" + "-" * 80)
            print(f"NEW RECORDS TO BE INSERTED ({len(changes['new'])}):")
            print("-" * 80)
            for record in changes['new']:
                snatch = str(record['snatch_record']) if record['snatch_record'] else 'N/A'
                cj = str(record['cj_record']) if record['cj_record'] else 'N/A'
                total = str(record['total_record']) if record['total_record'] else 'N/A'
                print(f"  {record['age_category']:8} {record['gender']:1} {record['weight_class']:10} | "
                      f"Snatch: {snatch:>6} | C&J: {cj:>6} | Total: {total:>6}")
        
        if changes['modified']:
            print("\n" + "-" * 80)
            print(f"MODIFIED RECORDS TO BE UPDATED ({len(changes['modified'])}):")
            print("-" * 80)
            for item in changes['modified']:
                record = item['record']
                old = item['old']
                print(f"  {record['age_category']:8} {record['gender']:1} {record['weight_class']:10}")
                
                if old.get('snatch_record') != record.get('snatch_record'):
                    old_val = old.get('snatch_record') or 'N/A'
                    new_val = record.get('snatch_record') or 'N/A'
                    print(f"    Snatch: {old_val} ‚Üí {new_val}")
                if old.get('cj_record') != record.get('cj_record'):
                    old_val = old.get('cj_record') or 'N/A'
                    new_val = record.get('cj_record') or 'N/A'
                    print(f"    C&J:    {old_val} ‚Üí {new_val}")
                if old.get('total_record') != record.get('total_record'):
                    old_val = old.get('total_record') or 'N/A'
                    new_val = record.get('total_record') or 'N/A'
                    print(f"    Total:  {old_val} ‚Üí {new_val}")
        
        if not changes['new'] and not changes['modified']:
            print("\n‚úÖ No changes detected - all records are up to date!")
        
        print("\n" + "=" * 80)
    
    def upsert_to_supabase(self, records: List[Dict]) -> Dict:
        """Upsert records to Supabase table"""
        if not self.supabase_url or not self.supabase_key:
            return {"status": "skipped", "message": "Supabase credentials not configured"}
        
        try:
            from supabase import create_client
            
            client = create_client(self.supabase_url, self.supabase_key)
            table_name = 'world_records'
            
            # Get existing records for comparison
            existing_records = self.get_existing_records()
            changes = self.compare_records(records, existing_records)
            
            # If dry run, just show what would happen
            if self.dry_run:
                self.print_dry_run_summary(records, changes)
                return {
                    "status": "dry_run",
                    "records_upserted": 0,
                    "message": f"DRY RUN: Would upsert {len(records)} records "
                              f"({len(changes['new'])} new, {len(changes['modified'])} modified)",
                    "changes": changes
                }
            
            # Clear existing records
            print("Clearing existing records...")
            client.table(table_name).delete().neq('id', 0).execute()
            
            # Insert new records
            print(f"Inserting {len(records)} records...")
            client.table(table_name).insert(records).execute()
            
            return {
                "status": "success",
                "records_upserted": len(records),
                "message": f"Successfully upserted {len(records)} records",
                "changes": changes
            }
            
        except ImportError:
            return {"status": "error", "message": "supabase-py library not installed"}
        except Exception as e:
            return {"status": "error", "message": f"Supabase error: {str(e)}"}
    
    def send_discord_notification(self, records_count: int, upsert_result: Dict) -> bool:
        """Send Discord notification with results"""
        if not self.discord_webhook:
            print("‚ö†Ô∏è  Discord webhook not configured")
            return False
        
        # Don't send notifications in dry run mode
        if self.dry_run:
            print("‚ö†Ô∏è  Skipping Discord notification in dry run mode")
            return False
        
        try:
            success = upsert_result.get('status') == 'success'
            
            if success:
                title = "‚úÖ IWF World Records Updated Successfully"
                description = "Daily world records scrape completed."
                color = 0x00ff00  # Green
            else:
                title = "‚ö†Ô∏è IWF World Records Update Issue"
                description = upsert_result.get('message', 'Unknown error')
                color = 0xffa500  # Orange
            
            embed = {
                "title": title,
                "description": description,
                "color": color,
                "timestamp": datetime.utcnow().isoformat(),
                "fields": [
                    {
                        "name": "Records Scraped",
                        "value": str(records_count),
                        "inline": True
                    },
                    {
                        "name": "Records Upserted",
                        "value": str(upsert_result.get('records_upserted', 0)),
                        "inline": True
                    },
                    {
                        "name": "Status",
                        "value": upsert_result.get('message', 'Unknown'),
                        "inline": False
                    }
                ]
            }
            
            data = {"embeds": [embed]}
            response = requests.post(self.discord_webhook, json=data)
            response.raise_for_status()
            
            print("‚úÖ Discord notification sent")
            return True
            
        except Exception as e:
            print(f"‚ùå Discord notification failed: {e}")
            return False
    
    def run_pipeline(self) -> int:
        """Execute the complete scraping pipeline"""
        print("=" * 60)
        print("IWF World Records Scraper Pipeline")
        if self.dry_run:
            print("üîç DRY RUN MODE ENABLED")
        print("=" * 60)
        
        # Step 1: Scrape records
        step_num = 1
        total_steps = 2 if self.dry_run else 4
        
        print(f"\n[{step_num}/{total_steps}] Scraping world records...")
        try:
            records = self.scrape_all()
            
            if not records:
                print("‚ùå No records found!")
                return 1
            
            print(f"‚úÖ Scraped {len(records)} records")
            
        except Exception as e:
            print(f"‚ùå Error during scraping: {e}")
            return 1
        
        step_num += 1
        
        # Step 2: Compare with Supabase (dry run) or Save CSV (normal mode)
        if self.dry_run:
            print(f"\n[{step_num}/{total_steps}] Comparing with Supabase...")
            upsert_result = self.upsert_to_supabase(records)
            
            if upsert_result['status'] == 'dry_run':
                print(f"‚úÖ {upsert_result['message']}")
            elif upsert_result['status'] == 'skipped':
                print(f"‚ö†Ô∏è  {upsert_result['message']}")
            else:
                print(f"‚ùå {upsert_result['message']}")
        else:
            # Normal mode: save CSV, upsert, and notify
            print(f"\n[{step_num}/{total_steps}] Saving to CSV...")
            try:
                self.save_to_csv(records)
            except Exception as e:
                print(f"‚ö†Ô∏è  Error saving CSV: {e}")
            
            step_num += 1
            
            # Step 3: Upsert to Supabase
            print(f"\n[{step_num}/{total_steps}] Upserting to Supabase...")
            upsert_result = self.upsert_to_supabase(records)
            
            if upsert_result['status'] == 'success':
                print(f"‚úÖ {upsert_result['message']}")
            elif upsert_result['status'] == 'skipped':
                print(f"‚ö†Ô∏è  {upsert_result['message']}")
            else:
                print(f"‚ùå {upsert_result['message']}")
            
            step_num += 1
            
            # Step 4: Send Discord notification
            print(f"\n[{step_num}/{total_steps}] Sending Discord notification...")
            self.send_discord_notification(len(records), upsert_result)
        
        # Summary
        print("\n" + "=" * 60)
        print("Pipeline Complete!")
        print("=" * 60)
        print(f"Records scraped: {len(records)}")
        if not self.dry_run:
            print(f"Supabase status: {upsert_result['status']}")
        print("=" * 60)
        
        return 0


def main():
    """Main entry point"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='IWF World Records Scraper - Scrapes, stores, and notifies about world records'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='Run in dry-run mode: scrape and compare with DB but do not write changes'
    )
    
    args = parser.parse_args()
    
    scraper = IWFWorldRecordsScraper(dry_run=args.dry_run)
    return scraper.run_pipeline()


if __name__ == "__main__":
    sys.exit(main())

